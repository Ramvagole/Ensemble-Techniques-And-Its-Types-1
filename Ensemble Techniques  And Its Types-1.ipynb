{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b174133-8c40-4e04-8c94-4ef5775d542a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1):-\n",
    "Ensemble techniques in machine learning are strategies that combine the predictions of multiple individual models (often referred to as \"base models\"\n",
    "or \"weak learners\") to create a more accurate and robust prediction. The idea behind ensemble methods is to leverage the diversity among the base \n",
    "models to improve overall performance and reduce the risk of overfitting.\n",
    "\n",
    "There are several popular ensemble techniques, including:\n",
    "\n",
    "Bagging (Bootstrap Aggregating):\n",
    "Bagging involves training multiple instances of the same base model on different subsets of the training data. Each subset is created through random \n",
    "sampling with replacement (bootstrap samples).\n",
    "The final prediction is often the average (for regression) or majority vote (for classification) of the predictions made by the individual models.\n",
    "Random Forest is a well-known ensemble method that uses bagging with decision trees as base models.\n",
    "\n",
    "Boosting:\n",
    "Boosting algorithms aim to sequentially train base models, where each subsequent model focuses on the data points that were misclassified \n",
    "by the previous models.\n",
    "Popular boosting algorithms include AdaBoost, Gradient Boosting, and XGBoost.\n",
    "Stacking (Stacked Generalization):\n",
    "\n",
    "Stacking combines predictions from multiple base models using a meta-model, which is trained on the predictions made by the base models.\n",
    "It involves a two-level process: the first level consists of the base models, and the second level is the meta-model.\n",
    "Stacking can be more complex to implement but often leads to improved performance.\n",
    "Voting:\n",
    "\n",
    "Voting ensembles combine predictions by taking a majority vote (for classification) or averaging (for regression) from multiple base models.\n",
    "It can be implemented in different ways, such as hard voting (simple majority) or soft voting (weighted average based on confidence scores).\n",
    "Blending:\n",
    "\n",
    "Blending is similar to stacking but is simpler. It involves training multiple models on the same dataset and then combining their predictions directly,\n",
    "typically with simple averaging or majority voting.\n",
    "Ensemble techniques are powerful because they can mitigate the weaknesses of individual models while harnessing their strengths. \n",
    "By combining multiple models, ensembles often yield better generalization, increased accuracy, and enhanced robustness in making predictions. \n",
    "However, they may require more computational resources and tuning compared to single models. The choice of ensemble technique and base models depends \n",
    "on the specific problem and dataset characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c53de0-f291-4d03-ac05-c8c65ce5369b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2):-\n",
    "Ensemble techniques are used in machine learning for several important reasons:\n",
    "Improved Predictive Performance: Ensemble methods can significantly enhance predictive performance compared to using a single base model.\n",
    "By combining multiple models, ensembles can capture different patterns and information in the data, leading to better generalization and more \n",
    "accurate predictions.\n",
    "\n",
    "Reduced Overfitting: Ensembles are less prone to overfitting compared to individual models. By aggregating the predictions of multiple base models,\n",
    "the ensemble can mitigate the noise and variance in the data, making it more robust to outliers and noisy examples.\n",
    "\n",
    "Enhanced Robustness: Ensembles are often more robust in handling different types of data and dataset characteristics. They can handle complex\n",
    "relationships and non-linearities that may be challenging for a single model to capture.\n",
    "\n",
    "Model Diversity: Ensemble methods encourage model diversity by training multiple base models using different subsets of data or different algorithms.\n",
    "This diversity helps ensure that errors made by one model are compensated for by others, leading to improved overall performance.\n",
    "\n",
    "Stability: Ensembles can provide stable and reliable predictions. Since they rely on the consensus of multiple models, they are less sensitive \n",
    "to small changes in the training data and can produce more consistent results.\n",
    "\n",
    "Versatility: Ensemble techniques can be applied to a wide range of machine learning algorithms, including decision trees, linear models, support \n",
    "vector machines, and neural networks. This versatility allows practitioners to leverage the strengths of various modeling approaches.\n",
    "\n",
    "Solving Complex Problems: For complex tasks and datasets, ensembles can be particularly effective. They are capable of handling intricate \n",
    "relationships and feature interactions that may be challenging to capture with a single model.\n",
    "\n",
    "Reduced Bias: Ensembles can reduce the bias of individual models. If a base model has a bias or systematic error, combining it with other models \n",
    "that have different biases can help mitigate this issue.\n",
    "\n",
    "Outlier Detection: Ensembles can be used for outlier detection by identifying data points where there is low agreement among the base models. \n",
    "Outliers are often those data points that are difficult to predict and are detected when the ensemble's predictions have high variance.\n",
    "\n",
    "Interpretability: In some cases, ensembles can provide insights into feature importance and model behavior by analyzing the contribution of different\n",
    "base models to the final prediction.\n",
    "Overall, ensemble techniques are a valuable tool in machine learning because they leverage the collective intelligence of multiple models to make\n",
    "better predictions, improve model stability, and handle a wide range of complex and challenging tasks. When used appropriately, ensembles can be a\n",
    "powerful approach to achieving high-quality machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e3c08a-c58c-4934-947e-0efffae0bd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3):-\n",
    "Bagging, which stands for Bootstrap Aggregating, is an ensemble machine learning technique used to improve the accuracy and robustness of predictive\n",
    "models, especially when dealing with high-variance algorithms like decision trees. The primary idea behind bagging is to create multiple subsets of \n",
    "the training data through resampling (with replacement), train a separate base model on each subset, and then aggregate their predictions to make a \n",
    "final prediction. Bagging is commonly used with decision trees, but it can be applied to other base models as well.\n",
    "\n",
    "Here's how the bagging process works:\n",
    "Bootstrap Sampling: Given a dataset with N data points, bagging randomly selects N data points (samples) from the original dataset with replacement. \n",
    "This means that some data points may be selected multiple times, while others may not be selected at all, resulting in multiple slightly different\n",
    "subsets.\n",
    "\n",
    "Base Model Training: A base model (often a decision tree, but it can be any model) is trained on each of these bootstrapped subsets. Since each \n",
    "subset is slightly different due to the random sampling, each base model will be slightly different.\n",
    "\n",
    "Prediction Aggregation:\n",
    "For regression problems, the final prediction is typically obtained by averaging the predictions made by all base models.\n",
    "For classification problems, a majority vote is used to determine the final class label. Each base model \"votes\" for a class, and the class with \n",
    "the most votes becomes the final prediction.\n",
    "\n",
    "The key advantages of bagging include:\n",
    "Reduced Variance: Bagging reduces the variance of the model because each base model is trained on a slightly different dataset. \n",
    "When aggregated, these diverse models can provide more accurate and stable predictions.\n",
    "\n",
    "Improved Generalization: Bagging can lead to better generalization by reducing overfitting, especially for base models like decision trees\n",
    "that are prone to high variance.\n",
    "\n",
    "Robustness: It makes the model more robust to outliers and noisy data points since individual errors are likely to cancel each other out when\n",
    "combining predictions.\n",
    "\n",
    "Parallelization: Bagging allows for parallel model training since each base model can be trained independently on a separate subset of the data, \n",
    "making it suitable for distributed computing environments.\n",
    "\n",
    "A well-known example of bagging is the Random Forest algorithm, which employs bagging with decision trees as base models. Random Forests are widely\n",
    "used in practice for tasks like classification and regression due to their high predictive accuracy and robustness.\n",
    "\n",
    "In summary, bagging is a powerful ensemble technique that leverages the diversity of multiple models trained on bootstrapped subsets of data to reduce\n",
    "variance, improve predictive performance, and enhance the robustness of machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681da97a-c630-436f-a674-54fb44ebd51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4):-\n",
    "Boosting is an ensemble machine learning technique used to improve the accuracy of predictive models, especially when dealing with weak learners\n",
    "(models that perform slightly better than random chance). Unlike bagging, which trains multiple base models independently and combines their \n",
    "predictions, boosting builds an ensemble of models sequentially, with each new model giving more attention to the data points that previous models \n",
    "found difficult to predict correctly. The primary idea behind boosting is to correct the errors made by earlier models, ultimately creating a strong \n",
    "learner from a collection of weak learners.\n",
    "\n",
    "Here's how the boosting process works:\n",
    "Base Model Training: Boosting starts by training a base model (often a simple one) on the entire training dataset.\n",
    "This initial model might make errors, but it serves as the starting point for the ensemble.\n",
    "\n",
    "Weighted Data: Each data point in the training set is assigned a weight. Initially, all weights are typically set equally.\n",
    "\n",
    "Sequential Model Building:\n",
    "Boosting iteratively builds a sequence of models.\n",
    "\n",
    "In each iteration, the algorithm:\n",
    "a. Focuses on the data points that were misclassified (or had higher errors) by the previous models.\n",
    "These data points are assigned higher weights, making them more important for the next model.\n",
    "b. Trains a new base model on the weighted dataset. This model is designed to correct the errors made by the previous models.\n",
    "c. Updates the ensemble by adding the new model, typically with a weight indicating its performance.\n",
    "d. Adjusts the data point weights based on the accuracy of the new model, giving more weight to the points it misclassified.\n",
    "This process continues for a predefined number of iterations or until some stopping criteria are met.\n",
    "Final Prediction: To make a final prediction, boosting combines the predictions of all base models, often weighted by their performance during \n",
    "training.\n",
    "\n",
    "Key advantages of boosting include:\n",
    "Improved Accuracy: Boosting can significantly improve model accuracy by iteratively focusing on the most challenging data points,\n",
    "effectively learning from its mistakes and reducing errors over time.\n",
    "\n",
    "Model Robustness: Boosting tends to create robust models that are less prone to overfitting, even when using complex base models.\n",
    "\n",
    "Adaptive Learning: It adapts to the difficulty of the dataset, assigning more importance to challenging data points and less to easy ones.\n",
    "\n",
    "Suitability for Weak Learners: Boosting is particularly effective when combined with weak learners, as it can turn them into strong learners \n",
    "through the ensemble process.\n",
    "\n",
    "Feature Selection: Some boosting algorithms can implicitly perform feature selection by giving more importance to relevant features during the\n",
    "boosting process.\n",
    "\n",
    "Common boosting algorithms include AdaBoost (Adaptive Boosting), Gradient Boosting, XGBoost, and LightGBM, each with its variations and strengths.\n",
    "These algorithms are widely used in both classification and regression tasks in machine learning due to their ability to create highly accurate and \n",
    "robust models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fc2606-544d-49ea-b47b-df5273c447c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5):-\n",
    "Ensemble techniques offer several benefits in machine learning:\n",
    "\n",
    "Improved Predictive Performance: One of the primary advantages of ensemble methods is that they often lead to improved predictive performance. \n",
    "By combining the predictions of multiple models, ensembles can capture a broader range of patterns and information in the data, resulting in more \n",
    "accurate and robust predictions.\n",
    "\n",
    "Reduced Overfitting: Ensembles are less prone to overfitting compared to individual models. The diversity among base models and the aggregation of\n",
    "their predictions help mitigate the noise and variance in the data, making the ensemble more robust to outliers and noisy examples.\n",
    "\n",
    "Enhanced Robustness: Ensemble techniques are typically more robust in handling different types of data and dataset characteristics. They can handle \n",
    "complex relationships and non-linearities that may be challenging for a single model to capture.\n",
    "\n",
    "Model Diversity: Ensembles encourage model diversity by training multiple base models using different subsets of data or different algorithms. \n",
    "This diversity helps ensure that errors made by one model are compensated for by others, leading to improved overall performance.\n",
    "\n",
    "Stability: Ensembles can provide stable and reliable predictions. Since they rely on the consensus of multiple models, they are less sensitive to\n",
    "small changes in the training data and can produce more consistent results.\n",
    "\n",
    "Versatility: Ensemble techniques can be applied to a wide range of machine learning algorithms, including decision trees, linear models, support\n",
    "vector machines, and neural networks. This versatility allows practitioners to leverage the strengths of various modeling approaches.\n",
    "\n",
    "Solving Complex Problems: For complex tasks and datasets, ensembles can be particularly effective. They are capable of handling intricate\n",
    "relationships and feature interactions that may be challenging to capture with a single model.\n",
    "\n",
    "Reduced Bias: Ensembles can reduce the bias of individual models. If a base model has a bias or systematic error, combining it with other models\n",
    "that have different biases can help mitigate this issue.\n",
    "\n",
    "Outlier Detection: Ensembles can be used for outlier detection by identifying data points where there is low agreement among the base models. \n",
    "Outliers are often those data points that are difficult to predict and are detected when the ensemble's predictions have high variance.\n",
    "\n",
    "Interpretability: In some cases, ensembles can provide insights into feature importance and model behavior by analyzing the contribution of different\n",
    "base models to the final prediction.\n",
    "\n",
    "Model Combination: Ensembles allow the combination of complementary models, each specialized in a particular aspect of the problem. This can lead to\n",
    "a more holistic understanding of the data.\n",
    "\n",
    "State-of-the-Art Performance: Many state-of-the-art machine learning models and competition-winning solutions in various domains use ensemble\n",
    "techniques to achieve the highest performance levels.\n",
    "\n",
    "In summary, ensemble techniques are a valuable tool in machine learning because they leverage the collective intelligence of multiple models to make\n",
    "better predictions, improve model stability, and handle a wide range of complex and challenging tasks. When used appropriately, ensembles can be a\n",
    "powerful approach to achieving high-quality machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaba21c-8dda-49bb-8386-49b64cd06d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6):-\n",
    "Ensemble techniques are powerful tools in machine learning and often lead to improved predictive performance compared to individual models, \n",
    "especially when used correctly. However, whether ensemble techniques are always better than individual models depends on various factors and\n",
    "considerations:\n",
    "\n",
    "Data Quality: Ensembles are most effective when the base models are diverse and capable of making different types of errors. If the data quality \n",
    "is poor, and all models in the ensemble are equally affected by the same noise or errors, then ensembles may not provide significant benefits.\n",
    "\n",
    "Computational Resources: Building and training an ensemble can be computationally expensive, especially if the base models are complex or if the \n",
    "dataset is large. In situations where computational resources are limited, using a single, well-tuned model might be more practical.\n",
    "\n",
    "Model Selection: The choice of base models and their hyperparameters can significantly impact the performance of an ensemble. If the wrong base\n",
    "models are selected or if they are not properly tuned, the ensemble may not outperform a well-tuned individual model.\n",
    "\n",
    "Interpretability: Ensembles are often less interpretable than individual models. If interpretability is a crucial requirement for a particular\n",
    "application, using a single model might be preferred.\n",
    "\n",
    "Problem Complexity: For simple and straightforward problems, using a single model may be sufficient and more straightforward. Ensembles are generally\n",
    "more beneficial for complex problems with intricate relationships and challenges that individual models struggle to address.\n",
    "\n",
    "Training Data Size: In cases where the training dataset is small, it may be challenging to build an ensemble effectively. Ensembles tend to shine when\n",
    "there is sufficient data to support the diversity of base models.\n",
    "\n",
    "Overfitting: While ensembles are less prone to overfitting than individual models, it's still possible for an ensemble to overfit the training data, \n",
    "especially if not properly regularized or if too many base models are used.\n",
    "\n",
    "Time Constraints: Building and training an ensemble can take more time than training a single model. If real-time or near-real-time predictions are \n",
    "required, ensembles may not be practical.\n",
    "\n",
    "Domain Expertise: In some cases, domain-specific knowledge and feature engineering may allow for the creation of a single highly effective model. \n",
    "Ensembles should be considered when there is uncertainty about which model or approach is best suited to the problem.\n",
    "\n",
    "In practice, it's advisable to consider both individual models and ensemble methods during the model selection and evaluation process. Start by\n",
    "training and evaluating individual models to establish a baseline. Then, experiment with different ensemble techniques and assess whether they lead\n",
    "to improvements in predictive performance, generalization, or robustness. The choice of whether to use an ensemble or an individual model should be\n",
    "based on empirical results and a thorough understanding of the problem and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4d3c1a-b0cc-49bd-9d38-2be85253ad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7):-\n",
    "The confidence interval using bootstrap resampling is a statistical technique that allows you to estimate the uncertainty or variability in a \n",
    "sample statistic (such as the mean, median, or any other parameter) by repeatedly resampling the data with replacement and then calculating the\n",
    "statistic of interest for each resampled dataset. The resulting distribution of the statistic is then used to construct a confidence interval.\n",
    "\n",
    "Here are the steps to calculate a confidence interval using the bootstrap method:\n",
    "Collect Your Data: Start with your original dataset, which you want to analyze and from which you want to estimate a statistic, such as the mean or \n",
    "median.\n",
    "\n",
    "Resample with Replacement: Generate a large number of bootstrap samples by randomly selecting data points from your original dataset with replacement.\n",
    "Each bootstrap sample should have the same size as your original dataset.\n",
    "\n",
    "Calculate Statistic: For each bootstrap sample, calculate the statistic of interest (e.g., mean, median, standard deviation, etc.). This gives you a\n",
    "collection of statistics, one for each bootstrap sample.\n",
    "\n",
    "Create Sampling Distribution: The collection of statistics from step 3 forms a sampling distribution of the statistic. This distribution represents \n",
    "the variability that would be observed if you were to repeatedly sample from the population.\n",
    "\n",
    "Determine Confidence Level: Choose your desired confidence level, typically expressed as a percentage (e.g., 95% confidence interval).\n",
    "\n",
    "Find Percentiles: To create the confidence interval, find the lower and upper percentiles of the sampling distribution that correspond to the chosen \n",
    "confidence level. For a 95% confidence interval, you would typically use the 2.5th percentile as the lower bound and the 97.5th percentile as the\n",
    "upper bound.\n",
    "\n",
    "Construct the Interval: The lower and upper percentiles from step 6 represent the lower and upper bounds of your confidence interval. You can now \n",
    "report this interval as your estimate of the parameter of interest, along with your chosen confidence level.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Your original dataset (replace with your data)\n",
    "original_data = np.array([...])\n",
    "\n",
    "# Number of bootstrap samples\n",
    "num_samples = 10000\n",
    "\n",
    "# Initialize an array to store bootstrap sample means\n",
    "bootstrap_means = np.empty(num_samples)\n",
    "\n",
    "# Perform bootstrap resampling\n",
    "for i in range(num_samples):\n",
    "    bootstrap_sample = np.random.choice(original_data, size=len(original_data), replace=True)\n",
    "    bootstrap_means[i] = np.mean(bootstrap_sample)\n",
    "\n",
    "# Calculate the lower and upper percentiles for the confidence interval\n",
    "confidence_level = 0.95\n",
    "lower_percentile = (1 - confidence_level) / 2\n",
    "upper_percentile = 1 - lower_percentile\n",
    "\n",
    "lower_bound = np.percentile(bootstrap_means, lower_percentile * 100)\n",
    "upper_bound = np.percentile(bootstrap_means, upper_percentile * 100)\n",
    "\n",
    "# The confidence interval\n",
    "print(f\"Bootstrap {confidence_level * 100}% Confidence Interval for Mean: ({lower_bound:.2f}, {upper_bound:.2f})\")\n",
    "\n",
    "In this example, were calculating a 95% confidence interval for the mean of the original data using bootstrap resampling.\n",
    "You can adapt this code for different statistics and confidence levels as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f40caff-ebb3-4535-bdc7-53edf4a26cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8):-\n",
    "Bootstrap is a statistical resampling technique used to estimate the sampling distribution of a statistic, such as the mean, median, variance,\n",
    "or other parameters, by repeatedly drawing random samples (with replacement) from a single dataset. It allows you to make inferences about the\n",
    "population parameter and quantify the uncertainty associated with the estimate. Here are the steps involved in the bootstrap procedure:\n",
    "\n",
    "Original Dataset: Start with your original dataset, which is assumed to be a representative sample from the population of interest. This dataset\n",
    "contains 'n' data points.\n",
    "\n",
    "Resampling: The core of the bootstrap method is to generate a large number of bootstrap samples by randomly selecting data points from the original\n",
    "dataset with replacement. Each bootstrap sample has the same size ('n') as the original dataset.\n",
    "\n",
    "Calculate Statistic: For each bootstrap sample, calculate the statistic of interest (e.g., mean, median, standard deviation, etc.). This step is \n",
    "essentially applying the same analysis to each of the resampled datasets.\n",
    "\n",
    "Repeat: Repeat step 3 a large number of times (e.g., thousands or tens of thousands of times). The number of bootstrap resamples is often denoted as \n",
    "'B'. This repetition is what allows you to create a distribution of the statistic.\n",
    "\n",
    "Sampling Distribution: The collection of statistics calculated from the bootstrap samples forms a sampling distribution of the statistic you are \n",
    "interested in. This distribution represents the variability in the estimate that you would observe if you were to repeatedly draw samples from the\n",
    "population.\n",
    "\n",
    "Confidence Interval: To construct a confidence interval for the parameter of interest, determine the percentiles of the sampling distribution.\n",
    "For example, to create a 95% confidence interval, you would find the 2.5th percentile as the lower bound and the 97.5th percentile as the upper bound.\n",
    "\n",
    "Reporting Results: Report the confidence interval along with the chosen confidence level, indicating the range within which you are reasonably\n",
    "confident the population parameter lies.\n",
    "\n",
    "The key idea behind bootstrap is that by resampling with replacement from the original dataset, you create a distribution of statistics that mimics\n",
    "the behavior of the statistic in the population. This allows you to estimate the parameter's value and its associated uncertainty without the need for \n",
    "assuming specific parametric distributions. Bootstrap is particularly useful when the underlying population distribution is unknown or complicated.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Your original dataset (replace with your data)\n",
    "original_data = np.array([...])\n",
    "\n",
    "# Number of bootstrap samples\n",
    "num_samples = 10000\n",
    "\n",
    "# Initialize an array to store bootstrap sample means\n",
    "bootstrap_means = np.empty(num_samples)\n",
    "\n",
    "# Perform bootstrap resampling\n",
    "for i in range(num_samples):\n",
    "    bootstrap_sample = np.random.choice(original_data, size=len(original_data), replace=True)\n",
    "    bootstrap_means[i] = np.mean(bootstrap_sample)\n",
    "\n",
    "# Calculate the 95% confidence interval for the mean\n",
    "lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "# The 95% confidence interval for the mean\n",
    "print(f\"Bootstrap 95% Confidence Interval for Mean: ({lower_bound:.2f}, {upper_bound:.2f})\")\n",
    "\n",
    "This code performs bootstrap resampling to estimate the mean of the original dataset and constructs a 95% confidence interval for the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7eb9ab-bf48-4be0-bb44-4aa340446991",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q9):-\n",
    "import numpy as np\n",
    "\n",
    "# Original sample data (mean and standard deviation)\n",
    "sample_mean = 15  # meters\n",
    "sample_std_dev = 2  # meters\n",
    "sample_size = 50\n",
    "\n",
    "# Number of bootstrap samples\n",
    "num_bootstrap_samples = 10000\n",
    "\n",
    "# Initialize an array to store bootstrap sample means\n",
    "bootstrap_sample_means = np.empty(num_bootstrap_samples)\n",
    "\n",
    "# Perform bootstrap resampling\n",
    "for i in range(num_bootstrap_samples):\n",
    "    # Generate a bootstrap sample by randomly selecting 50 trees with replacement\n",
    "    bootstrap_sample = np.random.normal(sample_mean, sample_std_dev, sample_size)\n",
    "    # Calculate the mean of the bootstrap sample\n",
    "    bootstrap_sample_means[i] = np.mean(bootstrap_sample)\n",
    "\n",
    "# Calculate the 95% confidence interval for the population mean\n",
    "confidence_interval = np.percentile(bootstrap_sample_means, [2.5, 97.5])\n",
    "\n",
    "# Print the confidence interval\n",
    "print(f\"Bootstrap 95% Confidence Interval for Population Mean Height: ({confidence_interval[0]:.2f} meters, {confidence_interval[1]:.2f} meters)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
